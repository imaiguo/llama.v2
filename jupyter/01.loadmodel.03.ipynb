{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f955897-59b8-4204-864f-56e446dca819",
   "metadata": {},
   "source": [
    "# llama2-wrapper\n",
    "\n",
    "在线指导 https://pypi.org/project/llama2-wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd5c5d4-9aec-4f90-aa10-88ea22c50ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    MODELPATH = \"E:/THUDM/llama2/model/llama-2-7b-chat\"\n",
    "    tokenizer_path = \"E:/THUDM/llama2/model/tokenizer.model\"\n",
    "else:\n",
    "    MODELPATH = \"/opt/Data/THUDM/llama2.hf/llama-2-7b-chat-hf\"\n",
    "    tokenizerPath = \"/opt/Data/THUDM/llama2/tokenizer.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74037a63-bba9-4bfd-b8af-801ca7609ec9",
   "metadata": {},
   "source": [
    "# 1 权重加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2b1627-2da2-4fa2-bbc5-87d66cbb9abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU with backend torch transformers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0419910bd6a4622ae8460418951f152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/Data/THUDM/llama2.hf/llama-2-7b-chat-hf and are newly initialized: ['model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from llama2_wrapper import LLAMA2_WRAPPER, get_prompt, get_prompt_for_dialog\n",
    "\n",
    "llm = LLAMA2_WRAPPER(\n",
    "\tmodel_path = MODELPATH,\n",
    "    backend_type = \"transformers\",\n",
    "    # load_in_8bit = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473909ea-8122-45af-be21-fda9abcba362",
   "metadata": {},
   "source": [
    "# 2 completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b064e2d-b4a6-4945-8e07-ba2b858d55ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Data/PythonVenv/llama2/lib/python3.11/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Great to hear! It's important to take care of yourself and prioritize your well-being. Is there anything in particular that you're feeling good about today?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I feel good.\"\n",
    "answer = llm(get_prompt(prompt), temperature=0.9)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c06806-37ba-4eca-a77b-791da3dc286e",
   "metadata": {},
   "source": [
    "# 3 chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fa3bba2-6cdf-4514-b971-9f071d48a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = [\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are a helpful, respectful and honest assistant. \"\n",
    "    },{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Hi do you know Pytorch?\",\n",
    "    },\n",
    "]\n",
    "result = llm.chat_completion(dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13c22f3f-f5f8-4786-b862-d27d94edc42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! Yes, I'm familiar with PyTorch. PyTorch is an open-source deep learning framework that is widely used for developing and training neural networks. It is known for its simplicity, flexibility, and ease of use, making it a popular choice among researchers and developers.\n",
      "PyTorch is built on top of the Python programming language and provides a dynamic computation graph, which allows for more flexible and efficient computation during training. It also has a Pythonic API, which makes it easy to use and integrate with other Python libraries.\n",
      "Some of the key features of PyTorch include:\n",
      "\n",
      "* Dynamic computation graph: PyTorch's computation graph is dynamic, which means that it can be built and modified during runtime. This allows for more flexible and efficient computation during training.\n",
      "* Pythonic API: PyTorch's API is built on top of Python, which makes it easy to use and integrate with other Python libraries.\n",
      "* Tensor computation: PyTorch provides a tensor computation system that is similar to NumPy, which makes it easy to use for a wide range of deep learning algorithms.\n",
      "* Autograd: PyTorch provides an automatic differentiation system called Autograd, which allows for efficient computation of gradients during training.\n",
      "* Modules: PyTorch provides a number of pre-built modules for common deep learning tasks, such as image classification, object detection, and natural language processing.\n",
      "* Extensive community support: PyTorch has a large and active community of developers and users, which means that there are many resources available for learning and troubleshooting.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f97aa3-29e6-4b44-af78-3567c7b8fcd2",
   "metadata": {},
   "source": [
    "# 4 generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e28ba-2c0c-4e9a-9e8b-ee831d447630",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_prompt(\"Hi do you know Pytorch?\")\n",
    "for response in llm.generate(prompt):\n",
    "\tprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e005195-3b52-458a-946e-a255e9b4a409",
   "metadata": {},
   "source": [
    "# 5 run\n",
    "\n",
    "run() is similar to generate(), but run()can also accept chat_historyand system_prompt from the users.\n",
    "\n",
    "It will process the input message to llama2 prompt template with chat_history and system_prompt for a chatbot-like app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc7f86d-9bce-4bbb-a129-30acd37d746d",
   "metadata": {},
   "source": [
    "# 6 get_prompt\n",
    "\n",
    "get_prompt() will process the input message to llama2 prompt with chat_history and system_promptfor chatbot.\n",
    "\n",
    "By default, chat_history and system_prompt are empty and get_prompt() will add llama2 prompt template to your message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "824f83c6-e0ea-4642-aa71-4b7cdfa3e352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Hi do you know Pytorch? [/INST]\n"
     ]
    }
   ],
   "source": [
    "prompt = get_prompt(\"Hi do you know Pytorch?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a67d5067-cba6-4331-ad14-39718ceef640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. \n",
      "<</SYS>>\n",
      "\n",
      "Hi do you know Pytorch? [/INST]\n"
     ]
    }
   ],
   "source": [
    "prompt = get_prompt(\"Hi do you know Pytorch?\", system_prompt=\"You are a helpful, respectful and honest assistant. \")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaeb63e-3206-46a3-ba46-78a011a52885",
   "metadata": {},
   "source": [
    "get_prompt_for_dialog\n",
    "\n",
    "get_prompt_for_dialog() will process dialog (chat history) to llama2 prompt for OpenAI compatible API /v1/chat/completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d4efada-df91-481d-b141-d5fbb3e54b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. \n",
      "<</SYS>>\n",
      "\n",
      "Hi do you know Pytorch? [/INST] Yes, I know [INST] How are you? [/INST]\n"
     ]
    }
   ],
   "source": [
    "dialog = [\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are a helpful, respectful and honest assistant. \"\n",
    "    },{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Hi do you know Pytorch?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":\"Yes, I know\",\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"How are you?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = get_prompt_for_dialog(dialog)\n",
    "\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
