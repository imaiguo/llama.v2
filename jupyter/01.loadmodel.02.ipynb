{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848d6ba6-8726-41da-a426-019d07fe29c3",
   "metadata": {},
   "source": [
    "# Hugging face\n",
    "\n",
    "从meta公司下载参数为bfloat16, 需要转换为fb16格式才能被Hugging face框架使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1242d78a-39ef-4ab5-a08a-f2cc6d268d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    MODEL_PATH = \"E:/THUDM/llama2.hf/llama-2-7b-chat\"\n",
    "else:\n",
    "    MODEL_PATH = \"/opt/Data/THUDM/llama2.hf/llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf9f3a0-91e6-4349-bdcb-9f8ad73aedb0",
   "metadata": {},
   "source": [
    "# 1 LlamaForCausalLM LlamaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed578a82-e64c-4b9e-874e-02aad2d3ba49",
   "metadata": {},
   "source": [
    "## 1.1 CPU计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e30ddb0-cc47-4ff2-99e9-0e544a01e0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4deabb6a17c4bb0b476b987032036d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/main_classes/model\n",
    "# torch_dtype\n",
    "# torch.float16 or torch.bfloat16 or torch.float: load in a specified dtype, ignoring the model’s config.torch_dtype if one exists. If not specified\n",
    "# the model will get loaded in torch.float (fp32).\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c1d3422-2796-4448-826c-6a8dfe8f7f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey, are you conscious? Can you talk to me?\\n\\nI'm just an AI, I don't have consciousness or the ability to talk like a human. I'm here to help answer your questions and provide information to the best of my ability. Is there something specific you would like to know or discuss?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=4096, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328080f-1bad-4112-b63c-7ccef1519266",
   "metadata": {},
   "source": [
    "## 1.2 GPU计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b40a992-c820-49c5-8edf-10e0a978a5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ad12b700524c80ab38471256e081b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08eb41f8-a40d-4701-9f8c-9d75e5d7770a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey, are you conscious? Can you talk to me?\\n\\nI'm just an AI, I don't have consciousness or the ability to talk like a human. I'm here to help answer your questions and provide information to the best of my ability. Is there something specific you would like to know or discuss?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "\n",
    "# Generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "generate_ids = model.generate(inputs.input_ids.cuda(), max_length=4096, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07698c-0c1d-4d3c-bcbc-c1b336cda9b6",
   "metadata": {},
   "source": [
    "## 1.3 TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdabea-86d9-4c24-a76b-ab7559d6700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM, TextStreamer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, add_prefix_space=True)\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16).to('cuda')\n",
    "model = model.eval()\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True, pad_token_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf637a-1056-4ed7-8358-fa0766946388",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n{} [/INST]\"\"\"\n",
    "\n",
    "prompt = instruction.format(\"what's the resulat of 10*4?\")\n",
    "generate_ids = model.generate(tokenizer(prompt, return_tensors='pt').input_ids.cuda(), max_new_tokens=4096, pad_token_id=tokenizer.eos_token_id, streamer=streamer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3651b1-54b3-4e04-9d75-dcd56aaf12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/LinkSoul-AI/Chinese-Llama-2-7b\n",
    "\n",
    "instruction = \"\"\"[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n{} [/INST]\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "prompt = instruction.format(\"When is the best time to visit Beijing, and do you have any suggestions for me?\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "generate_ids = model.generate(inputs.input_ids.cuda(), max_new_tokens=4096, pad_token_id=tokenizer.eos_token_id, streamer=streamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd89658-daf1-4271-bbd1-5039052a9250",
   "metadata": {},
   "source": [
    "# 2 transformers.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b97ac8-754a-4b6a-b478-4211f1e06785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe02a12b1e0d44c99d49a15ebffc5cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    # device_map=\"auto\",\n",
    "     # device_map=\"cpu\",\n",
    "    device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dfc5b64-6816-4b8c-afff-bd8659420ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "\n",
      "- TV Fan\n",
      "\n",
      "Hi TV Fan! Yes, I can definitely recommend some other shows that you might enjoy based on your preferences. Here are a few suggestions:\n",
      "\n",
      "1. \"The Sopranos\" - This classic HBO series is a crime drama that follows the life of a New Jersey mob boss, Tony Soprano, as he navigates the criminal underworld and deals with personal and family problems.\n",
      "2. \"The Wire\" - This HBO series explores the drug trade in Baltimore from multiple perspectives, including law enforcement, drug dealers, and politicians. It's a gritty and intense show that's sure to keep you on the edge of your seat.\n",
      "3. \"Narcos\" - This Netflix series tells the true story of Pablo Escobar, the infamous Colombian drug lord, and the DEA agents who hunted him down. It's a thrilling and action-packed show that's full of twists and turns.\n",
      "4. \"Mad Men\" - This AMC series is set in the 1960s and follows the lives of the employees at a New York advertising agency. It's a stylish and thought-provoking show that explores themes of identity, relationships, and the changing cultural landscape.\n",
      "5. \"The Shield\" - This FX series follows a corrupt cop and his team as they navigate the dangerous streets of Los Angeles. It's a gritty and intense show that explores themes of power, loyalty, and the blurred lines between good and evil.\n",
      "\n",
      "I hope these suggestions are helpful, TV Fan! Let me know if you have any other questions or if you'd like more recommendations.\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    max_length=4096,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
